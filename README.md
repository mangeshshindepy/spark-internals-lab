# ðŸ”¥ Spark Internals Lab

Deep dive into **Apache Spark internals** â€” understanding how Spark executes jobs under the hood, including **jobs, stages, tasks, shuffle, memory management, AQE, and execution plans**.

This repository is focused on **how Spark actually works**, not just how to use APIs.

---

## ðŸš€ Why this Repository?

Most Spark tutorials stop at:
```python
df.groupBy("col").count()

## Project Goals

This project goes deeper and answers the following questions:

- How many **jobs, stages, and tasks** are created?
- When and **why shuffle happens**
- How **memory is used** inside executors
- Why Spark **spills to disk**
- How **Spark 3 Adaptive Query Execution (AQE)** changes execution
- How to read the **Spark UI** and **execution plans**

---

## Who This Repository Is For

This repository is designed for:

- Data Engineers
- Big Data Engineers
- Senior Spark Developers
- Engineers preparing for **system design** and **performance interviews**

---

## ðŸ§  What You Will Learn

- âœ” Jobs vs Stages vs Tasks (with examples)
- âœ” Narrow vs Wide transformations
- âœ” Shuffle mechanics (map side & reduce side)
- âœ” Spark 2 vs Spark 3 execution differences
- âœ” Adaptive Query Execution (AQE)
- âœ” Executor & Driver memory internals
- âœ” Cache vs Persist
- âœ” Memory spill (execution & storage)
- âœ” How to read Spark UI correctly
- âœ” How to read Catalyst execution plans

